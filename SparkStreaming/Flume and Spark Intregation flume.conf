# example.conf: A single-node Flume configuration

# Name the components on this agent
sdc.sources = ws
sdc.sinks = hdf spark
sdc.channels = sparkmem hdmem

# Describe/configure the source
sdc.sources.ws.type = exec
sdc.sources.ws.command = tail -F /opt/gen_logs/logs/access.log

# Describe the HDFS sink
sdc.sinks.hdf.type = hdfs
sdc.sinks.hdf.hdfs.path = hdfs://nn01.itversity.com:8020/user/satya123zx/DeptCountFlume
sdc.sinks.hdf.hdfs.fileType = DataStream
sdc.sinks.hdf.hdfs.filePrefix = DeptCountFlumeFile
sdc.sinks.hdf.hdfs.fileSuffix = .txt
sdc.sinks.hdf.hdfs.rollInterval = 120
sdc.sinks.hdf.hdfs.rollSize = 1048575
sdc.sinks.hdf.hdfs.rollCount = 100

# Describe the spark sink

sdc.sinks.spark.type = org.apache.spark.streaming.flume.sink.SparkSink
sdc.sinks.spark.hostname = gw02.itversity.com
sdc.sinks.spark.port = 9943

# Use a channel which buffers events in memory
sdc.channels.sparkmem.type = memory
sdc.channels.sparkmem.capacity = 1000
sdc.channels.sparkmem.transactionCapacity = 100

sdc.channels.hdmem.type = memory
sdc.channels.hdmem.capacity = 1000
sdc.channels.hdmem.transactionCapacity = 100

# Bind the source and sink to the channel
sdc.sources.ws.channels = sparkmem hdmem
sdc.sinks.spark.channel = sparkmem
sdc.sinks.hdf.channel = hdmem

#After this we can save it and run it using flume-ng agent -n sdc -f dsc.conf